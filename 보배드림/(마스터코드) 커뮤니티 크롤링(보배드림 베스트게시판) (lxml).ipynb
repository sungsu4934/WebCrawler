{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html import fromstring\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "headers = {'User-Agent' : ua.random}\n",
    "\n",
    "# date 대체용: 오늘 월,일 정의 \n",
    "today_day = '%02d' % (datetime.datetime.now().day)\n",
    "today_month = '%02d' % (datetime.datetime.now().month)\n",
    "today_date = f'{today_month}/{today_day}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터프레임 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['제목','글쓴이','게시판','등록일','추천수','조회수','url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL 크롤링 (page만 변경할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "page = 1 ######### 수정 수집 시 여기만 변경\n",
    "date = today_date\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # page 바꾸기\n",
    "    base_url = f'https://www.bobaedream.co.kr/list?code=best&s_cate=&maker_no=&model_no=&or_gu=10&or_se=desc&s_selday=&pagescale=30&info3=&noticeShow=&s_select=&s_key=&level_no=&bestCode=&bestDays=&bestbbs=&vdate=&type=list&page={page}'\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(base_url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "        res.encoding = \"utf-8-sig\"\n",
    "        parser = fromstring(res.text)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            print(f'{page}페이지 에러발생!!! 5초를 쉽니다.')\n",
    "            time.sleep(5)\n",
    "            res = requests.get(base_url, headers=headers)\n",
    "            res.raise_for_status()\n",
    "            res.encoding = \"utf-8-sig\"\n",
    "            parser = fromstring(res.text)\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            try:\n",
    "                print(f'{page}페이지 2차 에러발생!!! 5초를 쉽니다.')\n",
    "                time.sleep(5)\n",
    "                res = requests.get(base_url, headers=headers)\n",
    "                res.raise_for_status()\n",
    "                res.encoding = \"utf-8-sig\"\n",
    "                parser = fromstring(res.text)\n",
    "                \n",
    "            except:\n",
    "                print(f'{page}페이지 3차 에러발생!!! 5초를 쉽니다.')\n",
    "                time.sleep(5)\n",
    "                res = requests.get(base_url, headers=headers)\n",
    "                res.raise_for_status()\n",
    "                res.encoding = \"utf-8-sig\"\n",
    "                parser = fromstring(res.text)\n",
    "            \n",
    "        \n",
    "\n",
    "    print(f'== {page}페이지 시작 == 경과시간: {round(time.time() - start)}초 == 현재 날짜: {date}')\n",
    "\n",
    "\n",
    "    for num in range(1, 31):\n",
    "\n",
    "        # find element\n",
    "        title = parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[2]/a[1]')[0].text_content()\n",
    "        author = parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[3]/span[2]')[0].text_content()\n",
    "        board = parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[1]')[0].text_content()\n",
    "        date = parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[4]')[0].text_content()\n",
    "        if ':' in date:\n",
    "            date = today_date\n",
    "        view = parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[6]')[0].text_content()\n",
    "        recommend = parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[5]/font[1]')[0].text_content()\n",
    "        url = 'https://www.bobaedream.co.kr'+parser.xpath(f'//*[@id=\"boardlist\"]/tbody/tr[{num}]/td[2]/a[1]')[0].attrib['href']\n",
    "\n",
    "        # append df\n",
    "        tmp = pd.DataFrame([[title, author, board, date, recommend, view, url]], columns = ['제목','글쓴이','게시판','등록일','추천수','조회수','url'])\n",
    "        df = df.append(tmp, ignore_index=True)\n",
    "        \n",
    "    # add page\n",
    "    page += 1\n",
    "    \n",
    "    # stopping condition\n",
    "    if date[:2] == '10': # 6개월치만 수집\n",
    "        print('수집종료')\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['내용'] = 0\n",
    "start = time.time()\n",
    "\n",
    "for idx in range(df.shape[0]):\n",
    "    \n",
    "    url = df.loc[idx, 'url']\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "        res.encoding = \"utf-8-sig\"\n",
    "        parser = fromstring(res.text)\n",
    "\n",
    "        title = parser.xpath('//*[@id=\"print_area\"]/div[1]/dl/dt/strong/text()')[0]\n",
    "        text = parser.xpath('//*[@id=\"print_area\"]/div[2]/div')[0].text_content()\n",
    "\n",
    "        df.loc[idx, ['제목', '내용']] = [title, text]\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        try:\n",
    "            print(f'{idx}번째 url 에러발생!!! 3초를 쉽니다.')\n",
    "            time.sleep(3)\n",
    "\n",
    "            res = requests.get(url, headers=headers)\n",
    "            res.raise_for_status()\n",
    "            res.encoding = \"utf-8-sig\"\n",
    "            parser = fromstring(res.text)\n",
    "\n",
    "            title = parser.xpath('//*[@id=\"print_area\"]/div[1]/dl/dt/strong/text()')[0]\n",
    "            text = parser.xpath('//*[@id=\"print_area\"]/div[2]/div')[0].text_content()\n",
    "\n",
    "            df.loc[idx, ['제목', '내용']] = [title, text]\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                print(f'{idx}번째 url 2차 에러발생!!! 3초를 쉽니다.')\n",
    "                time.sleep(3)\n",
    "\n",
    "                res = requests.get(url, headers=headers)\n",
    "                res.raise_for_status()\n",
    "                res.encoding = \"utf-8-sig\"\n",
    "                parser = fromstring(res.text)\n",
    "\n",
    "                title = parser.xpath('//*[@id=\"print_area\"]/div[1]/dl/dt/strong/text()')[0]\n",
    "                text = parser.xpath('//*[@id=\"print_area\"]/div[2]/div')[0].text_content()\n",
    "\n",
    "                df.loc[idx, ['제목', '내용']] = [title, text]\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    print(f'{idx}번째 url 3차 에러발생!!! 3초를 쉽니다.')\n",
    "                    time.sleep(3)\n",
    "\n",
    "                    res = requests.get(url, headers=headers)\n",
    "                    res.raise_for_status()\n",
    "                    res.encoding = \"utf-8-sig\"\n",
    "                    parser = fromstring(res.text)\n",
    "\n",
    "                    title = parser.xpath('//*[@id=\"print_area\"]/div[1]/dl/dt/strong/text()')[0]\n",
    "                    text = parser.xpath('//*[@id=\"print_area\"]/div[2]/div')[0].text_content()\n",
    "\n",
    "                    df.loc[idx, ['제목', '내용']] = [title, text]\n",
    "                    \n",
    "                except:\n",
    "                    \n",
    "                    print(f'=== {idx}번째 url은 삭제된 페이지로 판단되어 수집하지 않습니다. ===')\n",
    "        \n",
    "    if idx % 10 == 0:\n",
    "        print(f'== {idx}개 완료 == 경과시간: {round(time.time() - start)}초 == ')\n",
    "        \n",
    "    if idx % 1000 == 0:\n",
    "        df.to_csv(f'보배드림_베스트글_크롤링_본문까지({idx}까지).csv', index=False) # 중간저장을 하신다면, 경로만 옳게 바꿔줍니다.\n",
    "        print(f'== {idx}까지 중간저장 완료 ==')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
