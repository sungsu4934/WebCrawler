{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd \n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집하고자 하는 id 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_id_list = ['1106610041', '1078930793', '36543606'] # <네이버 지도> 크롤링을 통해 얻은 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "count = 1\n",
    "\n",
    "for place_id in place_id_list:\n",
    "\n",
    "    # 크롤링\n",
    "    url = 'https://m.place.naver.com/place/{}/home?entry=ple'.format(place_id) # 링크1\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout = 5) as response:\n",
    "            html = response.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                value7 = str(soup.find_all(\"span\", {\"class\": \"_2yqUQ\"})[0]).split('>')[1].split('<')[0]                \n",
    "                value8 = str(soup.find_all(\"span\", {\"class\": \"_3XamX\"})[0]).split('>')[1].split('<')[0]                \n",
    "                value9 = str(soup.find_all(\"span\", {\"class\": \"_3ocDE\"})[0]).split('>')[1].split('<')[0]                \n",
    "                \n",
    "            except: # 링크2\n",
    "                time.sleep(3)\n",
    "                url_2 = 'https://m.place.naver.com/accommodation/{}'.format(place_id)\n",
    "\n",
    "                with urllib.request.urlopen(url_2, timeout = 5) as response:\n",
    "                    html = response.read()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    try:\n",
    "                        value7 = str(soup.find_all(\"span\", {\"class\": \"_2yqUQ\"})[0]).split('>')[1].split('<')[0]                \n",
    "                        value8 = str(soup.find_all(\"span\", {\"class\": \"_3XamX\"})[0]).split('>')[1].split('<')[0]    \n",
    "                        value9 = str(soup.find_all(\"span\", {\"class\": \"_3ocDE\"})[0]).split('>')[1].split('<')[0]                \n",
    "                    except:\n",
    "                        print('=== placeid: {}는 url이 옳지 않습니다. ==='.format(place_id))\n",
    "                        error_place_id.append(place_id)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # append df\n",
    "    value_df = pd.DataFrame({'id':[place_id], '가맹점명':[value8], '주소': [value7], '카테고리':[value9]})\n",
    "    result = result.append(value_df, ignore_index=True)\n",
    "    count += 1\n",
    "\n",
    "    # print log\n",
    "    if count % 10 == 0:\n",
    "        t2 = time.time()\n",
    "        elapsed_time= round(t2-t1)\n",
    "        print('=== {}번째 placeid 완료 === 경과시간: {}초'.format(count, elapsed_time))\n",
    "\n",
    "# save dataframe\n",
    "result = result.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "print('크롤링완료!!!!')\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
