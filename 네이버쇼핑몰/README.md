## 네이버쇼핑몰을 크롤링하는 코드입니다. (lxml, selenium)

> 저는 네이버 쇼핑몰 리뷰를 크롤링하는 것에 초점을 맞추었습니다. NLP에 리뷰와 같은 데이터가 필요하신 분은 유용하게 쓰실 수 있을 것입니다.
 *https://search.shopping.naver.com/search/allfrm=NVSHMDL&origQuery=%EC%9A%B0%EC%9C%A0&pagingIndex=1&pagingSize=40&productSet=model&query=%EC%9A%B0%EC%9C%A0&sort=review&timestamp=&viewType=list*

> 네이버 쇼핑몰 리뷰 크롤링은 lxml로 url을 우선적으로 크롤링한 후 selenium으로 세부 리뷰들을 크롤링하는 식으로 진행됩니다.

> 수집하는 정보: 상품명, 상품등록일자, 리뷰개수, 구매업체, 리뷰, 리뷰작성자, 리뷰작성일 등,,,

## 수집과정 (1. url 수집)

Step1. 수집할 url의 형태는 다음과 같다. 각 수집할 때 주의해야할 점은 아래와 같다.

https://search.shopping.naver.com/search/all?exagency=true&frm=NVSHMDL&origQuery={KEYWORD}&pagingIndex={PAGE}&pagingSize=5&productSet=model&query={KEYWORD}&sort=review&timestamp=&viewType=list

  > a. KEYWORD는 라이브러리 내 세부 키워드들이 들어간다.
  > 
  > b. PAGE는 현재 아래 보이는 것과 같은 1~6를 조정하기 위해 들어간다.
  > 
  > c. pagingSize인자에 5를 대입하였다. 이는 내가 한 페이지에서 볼 수 있는 최대 항목수를 나타낸다. 스크롤을 내리지 않으면 최대 5개의 정보만 제공하여 pagingSize에 40을 넣더라도 5개의 정보만 수집된다. selenium으로 구현하여 스크롤을 내리게 할 수 있으나 스크롤을 내리는데 오류가 발생할 위험이 있고 lxml로 5개씩 여러 페이지를 수집하는 것이 빠르다고 판단하여 pagingSize는 5로 고정하였다.

Step2. 본격적인 수집
본격적으로 위 url에대해 정보를 수집해본다. 정보수집을 위한 알고리즘은 아래와 같이 작성하였다.



     for 키워드 in 검색하고자하는 단어들
  
         키워드 검색
         While True: # url 수집단계
           정보수집 (만약 리뷰개수가 0인 항목이 있다면 0을 채워줄 것)

           # Stopping condition 2가지
           If 상품명이 수집되지 못하였다면 --> 오류일 수 있으니 재시도
              재시도 후에도 안되면 정보가 없는 페이지로 판단하여 해당 키워드는 수집종료
              While문 break
           If 해당 url에 리뷰개수가 0인 항목을 발견 --> 이하 url은 수집하지 않음 
              *그 뒤 url은 모두 리뷰가 0개일 것이기 때문*

           데이터프레임에 url에서 수집한 데이터를 저장(5개씩)
 

 

-	수집할 대상은 “상품명, 등록일, 리뷰개수, 상품url, 검색키워드”이다. 

- 조기종료 조건은 2가지다. 

        a. 우선적으로 lxml에서 계속 url을 불러올 경우 순간 url을 받아오지 못할 때가 있다. 해당 경우를 판단하는 기준은 상품명이 제대로 수집되었나로 파악하였다. 
        이런 케이스에 대해 1차적으로 10초 휴식 후 다시 접근하도록 한다. 만약 재접근을 했음에도 상품명이 수집되지 않았다면 해당 페이지는 정말 상품이 없어서 
        상품명이 수집되지 않은 것으로 판단하여 조기종료를 실시한다.

        b. 우리는 페이지를 리뷰많은순으로 정렬하였다. 즉, 리뷰가 0개인 곳을 발견했다면 뒤 페이지는 수집할 필요가 없다. 
        리뷰가 0개인 페이지를 발견한다면 해당 키워드는 조기종료하고 다음 키워드로 넘어간다.


## 수집과정 (2. 정보수집)
> Step1. URL 1개씩 접속
> 
>> Part1에서 수집한 url을 한 개 씩 접근한다. 아래 보이는 코드를 통해 아래 보이는 사진의 위치로 이동한다. Selenium 이동 간 주의할 점은 click()을 위해서는 객체가 화면상에 드러나야 클릭이 가능하기에 충분한 sleep을 주도록한다. 결과적으로 빨간색 부분이 보인다면 해당 과정이 완료된다. 

> Step2. 각 점수별 리뷰 클릭(5점, 4점, 3점, 2점, 1점)
>> 각 점수별 리뷰를 클릭한다. 만약 3점리뷰가 0개일 때, 클릭 자체가 불가능하기에 try, except를 통한 예외처리가 필요하다. 

> Step3. 정보수집
>> get_info_about_detail()이라는 함수를 통해 정보를 수집한다. 수집 후 데이터프레임에 적재한다.

> Step4. 리뷰 내 페이지 이동
>> 모든 데이터가 적재되었다면 페이지를 이동한다. 페이지에 대한 예외는 5가지가 존재. (다음 유무, 이전 유무, 스크롤바가 아예 없을 떄)

>> 추가적으로, 예를 들어 5점 리뷰가 2400개인 경우, 네이버는 2000개까지(100페이지)만 제공한다. 이를 만약 100페이지라면, 해당 페이지까지만 수집하도록 예외 처리하였다.


#### [기타첨언]

1.	만약 모든 평점을 모두 수집한다면, 굳이 각 점수별 모든 점수의 리뷰를 클릭할 필요 없이 “전체”에 대해서만 진행한다면 더욱 빠르게 수집 가능할 것이다. 하지만 사고에 대한 리뷰를 수집하기 위해 1~3점만 수집하는 식으로 진행한다면 위 코드가 유용할 것

2.	Selenium 특성 상 안정적으로 진행하기 위해선 많은 sleep이 필요하다. 최소한 시간을 줄이기 위하여 error_url이 나오면 list형태로 저장하게 구현하였으며 우선적으로 모든 url에 대해 수집한 뒤, 추가적인 예외 케이스에 대해서만 다시 수집하는 방향으로 가는 것이 빠를 것.

3.	중간저장: 현재 코드상에는 url수집은 keyword단위로, 세부정보수집은 url 100개 단위로 진행하고 있다.

4.	1점에서 5점까지 모두 수집 시 현재 수집속도
리뷰: 1000개당 300초
세부정보: 1000개당 42,000초
--> 1점에서 3점만 수집한다면 훨씬 빠르게 수집할 수 있을 것
