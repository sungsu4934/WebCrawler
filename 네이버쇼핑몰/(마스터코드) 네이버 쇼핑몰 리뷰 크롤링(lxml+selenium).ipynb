{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import lxml\n",
    "from lxml.html import fromstring\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "url = \"주소\"\n",
    "headers = {'User-Agent' : ua.random}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['우유', '참외', '상추', '삼겹살']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL 크롤링 (by lxml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_about_url(base_url):\n",
    "    req = requests.get(base_url)\n",
    "    doc = fromstring(req.text)\n",
    "    productnames = [element.text_content() for element in doc.xpath('//*[@id=\"__next\"]/div/div[2]/div/div[3]/div[1]/ul/div/div/li/div/div[2]/div[1]/a')]\n",
    "    dates = [element.text_content().split(' ')[1] for element in doc.xpath('//*[@id=\"__next\"]/div/div[2]/div/div[3]/div[1]/ul/div/div/li/div/div[2]/div[5]/span[1]')]\n",
    "    review_nums = [element.text_content() for element in doc.xpath('//*[@id=\"__next\"]/div/div[2]/div/div[3]/div[1]/ul/div/div/li/div/div[2]/div[5]/a/em')]\n",
    "    urls = [element.get('href') for element in doc.xpath('//*[@id=\"__next\"]/div/div[2]/div/div[3]/div[1]/ul/div/div/li/div/div[2]/div[1]/a')]\n",
    "\n",
    "            \n",
    "    if len(review_nums) < 5:\n",
    "        while len(review_nums) != len(productnames):\n",
    "            review_nums.append('0')\n",
    "            \n",
    "    return productnames, dates, review_nums, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_url = pd.DataFrame()\n",
    "t1 = time.time()\n",
    "\n",
    "for idx, KEYWORD in enumerate(KEYWORDS):\n",
    "    PAGE = 1\n",
    "\n",
    "    while True:\n",
    "        base_url = f'https://search.shopping.naver.com/search/all?exagency=true&frm=NVSHMDL&origQuery={KEYWORD}&pagingIndex={PAGE}&pagingSize=5&productSet=model&query={KEYWORD}&sort=review&timestamp=&viewType=list'\n",
    "\n",
    "        # 정보수집\n",
    "        productnames, dates, review_nums, urls = get_info_about_url(base_url)\n",
    "        search_words = [KEYWORD] * len(productnames)\n",
    "\n",
    "        # Stopping condition\n",
    "        if len(productnames) == 0:\n",
    "            try:\n",
    "                print(f'== 키워드: {KEYWORD}의 {PAGE}페이지에 정보가 존재하지 않습니다. url 오류일 수 있으니 10초 후 재접근합니다.')\n",
    "                time.sleep(10)\n",
    "                productnames, dates, review_nums, urls = get_info_about_url(base_url)\n",
    "                search_words = [KEYWORD] * len(productnames)\n",
    "\n",
    "            except:\n",
    "                print(f'== 키워드: {KEYWORD}의 {PAGE}페이지부터 정보가 존재하지 않는 것으로 판단됩니다. 다음 키워드로 넘어갑니다. == 경과시간: {round(time.time()-t1)}초')\n",
    "                break\n",
    "\n",
    "        # Append df\n",
    "        tmp = pd.DataFrame({'상품명':productnames,\n",
    "                           '등록일':dates,\n",
    "                            '리뷰개수':review_nums,\n",
    "                           'url':urls,\n",
    "                           '검색키워드':search_words})\n",
    "        df_url = df_url.append(tmp, ignore_index=True)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        if '0' in review_nums:\n",
    "            print(f'{PAGE}페이지에 리뷰가 0개인 항목이 존재합니다. 이하 항목은 모두 리뷰가 0개로 판단되어 조기종료합니다.')\n",
    "            break\n",
    "\n",
    "        # log\n",
    "        if PAGE % 10 == 0:\n",
    "            print(f'== 키워드: {KEYWORD} == 페이지: {PAGE} 완료 == 경과시간: {round(time.time()-t1)}초')\n",
    "\n",
    "        # add PAGE\n",
    "        PAGE += 1\n",
    "\n",
    "    # log2\n",
    "    KEYWORD_NAME = KEYWORD.replace('/','')\n",
    "    df_url.to_csv(f'네이버쇼핑몰_url_{KEYWORD_NAME}까지.csv', index=False)\n",
    "    print(f'== 키워드: {KEYWORD} 완료({idx+1}/{len(KEYWORDS)})!! == 경과시간: {round(time.time()-t1)}초 == 중간저장완료\\n')\n",
    "    time.sleep(5)\n",
    "    \n",
    "df_url = df_url[df_url['리뷰개수'] != '0'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 세부정보수집 (selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_about_detail():\n",
    "\n",
    "    contents = [element.text for element in driver.find_elements_by_xpath('//*[@id=\"section_review\"]/ul/li/div[2]/div[1]/p')]\n",
    "    productnames = [df_url.loc[url_idx,'상품명']] * len(contents)\n",
    "    dates = [element.text for element in driver.find_elements_by_xpath('//*[@id=\"section_review\"]/ul/li/div[1]/span[4]')]\n",
    "    writers = [element.text for element in driver.find_elements_by_xpath('//*[@id=\"section_review\"]/ul/li/div[1]/span[3]')]\n",
    "    scores = [score] * len(contents)\n",
    "    headlines = [element.text for element in driver.find_elements_by_xpath('//*[@id=\"section_review\"]/ul/li/div[2]/div/em')]\n",
    "    urls = [url] * len(contents)\n",
    "    shoppingmalls = [element.text for element in driver.find_elements_by_xpath('//*[@id=\"section_review\"]/ul/li/div[1]/span[2]')]\n",
    "    search_words = [df_url.loc[url_idx, '검색키워드']] * len(contents)\n",
    "\n",
    "    tmp = pd.DataFrame({'작성자':writers,\n",
    "                           '작성일자':dates,\n",
    "                           '상품명':productnames,\n",
    "                           '평점':scores,\n",
    "                           '구매쇼핑몰':shoppingmalls,\n",
    "                           '제목':headlines,\n",
    "                           '본문':contents,\n",
    "                           'url':urls,\n",
    "                           '검색키워드':search_words})\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "def move_page(): # 다음페이지로 이동할 수 있도록 화면 이동\n",
    "    location = driver.find_element_by_xpath('//*[@id=\"section_review\"]/div[3]/a[1]')\n",
    "    driver.execute_script(f\"window.scrollTo(0, {location.location['y']-300})\") \n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(f'//*[@id=\"section_review\"]/div[3]/a[{page}]').click()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_url = pd.read_csv('../데이터/네이버쇼핑몰리뷰크롤링/네이버쇼핑몰_url_data_sample.csv')\n",
    "df_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "t1 = time.time()\n",
    "error_idx = []\n",
    "driver = webdriver.Chrome('chromedriver_91.exe', options=options)\n",
    "score_1_path = '//*[@id=\"section_review\"]/div[2]/div[2]/ul/li[6]/a' # 1점\n",
    "score_2_path = '//*[@id=\"section_review\"]/div[2]/div[2]/ul/li[5]/a' # 2점\n",
    "score_3_path = '//*[@id=\"section_review\"]/div[2]/div[2]/ul/li[4]/a' # 3점\n",
    "score_4_path = '//*[@id=\"section_review\"]/div[2]/div[2]/ul/li[3]/a' # 4점\n",
    "score_5_path = '//*[@id=\"section_review\"]/div[2]/div[2]/ul/li[2]/a' # 5점\n",
    "\n",
    "for url_idx in range(df_url.shape[0]):\n",
    "    \n",
    "    try:\n",
    "        # url 접속\n",
    "        url = df_url.loc[url_idx, 'url']\n",
    "        driver.get(url)    \n",
    "        print(f'\\n{url_idx+1}번째 url 시작 == 경과시간:{round(time.time()-t1)}초')\n",
    "\n",
    "        # 쇼핑몰 리뷰 있는 페이지까지 이동\n",
    "        location = driver.find_element_by_xpath('//*[@id=\"snb\"]/ul/li[2]/a')\n",
    "        driver.execute_script(f\"window.scrollTo(0, {location.location['y']-200})\") \n",
    "        time.sleep(1)\n",
    "\n",
    "        # 쇼핑몰리뷰 클릭\n",
    "        driver.find_element_by_xpath('//*[@id=\"snb\"]/ul/li[2]/a').click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 1점, 2점, 3점 위치로 이동\n",
    "        location = driver.find_element_by_xpath(score_1_path)\n",
    "        driver.execute_script(f\"window.scrollTo(0, {location.location['y']-200})\") \n",
    "\n",
    "        # 리뷰 크롤링\n",
    "        for score, score_element in zip([1,2,3], [score_1_path, score_2_path, score_3_path]):\n",
    "\n",
    "            try: # 1점, 2점, 3점 리뷰가 없을 수도 있다.\n",
    "                # 각 해당하는 점수를 클릭\n",
    "                location = driver.find_element_by_xpath(score_1_path)\n",
    "                driver.execute_script(f\"window.scrollTo(0, {location.location['y']-200})\") \n",
    "                time.sleep(0.5)\n",
    "                driver.find_element_by_xpath(score_element).click()\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                loop = True\n",
    "                page_count = 1\n",
    "\n",
    "                while (loop):\n",
    "\n",
    "                    # Case1. \"다음\"이 존재하는 것\n",
    "                    try: # 1page 단위에서 \"다음\"이 존재하는 것\n",
    "                        if driver.find_element_by_xpath(f'//*[@id=\"section_review\"]/div[3]/a[11]').text == '다음':\n",
    "                            for page in range(2, 12):\n",
    "                                tmp = get_info_about_detail()\n",
    "                                df_final = df_final.append(tmp, ignore_index=True)\n",
    "                                move_page()\n",
    "                                page_count += 1\n",
    "\n",
    "                        elif driver.find_element_by_xpath(f'//*[@id=\"section_review\"]/div[3]/a[12]').text == '다음': # 11~20, 21~30처럼 다음이 존재하는 경우\n",
    "                            for page in range(3, 13):\n",
    "                                tmp = get_info_about_detail()\n",
    "                                df_final = df_final.append(tmp, ignore_index=True)\n",
    "                                \n",
    "                                if driver.find_element_by_xpath(f'//*[@id=\"section_review\"]/div[3]/a[{page}]').text == '100':\n",
    "                                    print(f'리뷰개수가 100페이지에 도달하면 더이상 네이버에서 제공하지 않습니다.')\n",
    "                                    loop = False\n",
    "                                    break\n",
    "                                \n",
    "                                move_page()\n",
    "                                page_count += 1\n",
    "\n",
    "                    # Case2. \"다음\"이 존재하지 않는 경우\n",
    "                    except: \n",
    "\n",
    "                        try:\n",
    "                            if driver.find_element_by_xpath(f'//*[@id=\"section_review\"]/div[3]/a[1]').text == '이전': # 2-1. \"이전\"이 존재하는 경우 71~73같은 형태\n",
    "\n",
    "                                for page in range(3, 13):\n",
    "                                    try:\n",
    "                                        tmp = get_info_about_detail()\n",
    "                                        df_final = df_final.append(tmp, ignore_index=True)\n",
    "                                        move_page()\n",
    "                                        page_count += 1\n",
    "\n",
    "                                    except:\n",
    "                                        loop = False\n",
    "                                        break\n",
    "\n",
    "\n",
    "                            else: # 2-1. \"이전\"이 존재하지 않는 경우 1~8같은 형태\n",
    "                                for page in range(2, 12):\n",
    "                                    try:\n",
    "                                        tmp = get_info_about_detail()\n",
    "                                        df_final = df_final.append(tmp, ignore_index=True)\n",
    "                                        move_page()\n",
    "                                        page_count += 1\n",
    "\n",
    "                                    except:\n",
    "                                        loop = False\n",
    "                                        break\n",
    "\n",
    "                        except: # Case3. 1페이지만 있는 경우 상태바가 아예 없음\n",
    "                            tmp = get_info_about_detail()\n",
    "                            df_final = df_final.append(tmp, ignore_index=True)\n",
    "                            loop = False\n",
    "\n",
    "\n",
    "                print(f'{score}점 수집완료!')\n",
    "\n",
    "            except:\n",
    "                print(f'{score}점 리뷰가 존재하지 않습니다!')\n",
    "\n",
    "\n",
    "        if (url_idx+1) % 100 == 0:\n",
    "            df_final.to_csv(f'네이버쇼핑몰_세부정보_{url_idx+1}개까지.csv', index=False)\n",
    "    except:\n",
    "        print(f'== {url_idx+1}번째 url에 오류 발견 == 수집하지 않습니다. ==')\n",
    "        error_idx.append(url_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('../데이터/코티티_네이버쇼핑몰_크롤링_샘플.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
