## 네이버 뉴스 크롤링입니다. lxml로 진행하였습니다.
> 실행순서는 링크수집 --> 제목 및 본문수집 입니다.

#### 개요
NLP의 발전으로 텍스트 데이터의 수요가 급증하였습니다. 가장 정제되고 문법을 갖춘 텍스트라고 하면 대표적으로 뉴스가 있습니다. 본 챕터에서는 네이버 뉴스를 크롤링하고자 합니다.

#### 수집과정
1. 네이버 뉴스 url을 수집
2. 뉴스 제목 및 본문을 수집

#### 1. 네이버 뉴스 url을 수집
- url은 아래와 같습니다. 해당 url에는 {}가 2개 존재합니다. 첫 빈칸에는 검색어가 들어가고, 두 번째 빈칸에는 몇번째 element가 필요한지 들어갑니다.
> 네이버 뉴스는 한 페이지에 10개의 element를 제공합니다. 한 페이지를 수집하고 다음페이지를 수집하기 위해서는 element를 10단위로 추가해야합니다. (1, 11, 21 등,,)

*https://search.naver.com/search.naver?where=news&sm=tab_pge&query={}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=27&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start={}*

- 네이버 뉴스는 주의해야할 점이 있습니다. 아래 사진처럼 반드시 [네이버뉴스]가 활성화 된 사이트만 접속해야 일관된 포맷을 가져 데이터 수집이 가능합니다.

![image](https://user-images.githubusercontent.com/28617435/122548408-481f7a80-d06c-11eb-8dc9-329920e05899.png)


- 페이지를 넘김에 있어 주의해야할 점이 있습니다. "우유"키워드로 뉴스를 검색했을 때 7page까지 밖에 안 나왔다고 합시다. 7page는 start인자에 61을 입력하면 접근가능합니다.

> 이에 우리가 81로 접근하면 어떻게 될까요? 71페이지와 같은 결과를 계속 출력합니다. 하지만 for문을 생각없이 돌린다면 계속 같은 결과만 출력하며 시간낭비가 될 것입니다.
> 
>> 1. 이에 조기종료 조건을 삽입하였습니다. 7page의 마지막 요소의 url과 8page의 url요소를 비교한 후 동일하면 종료하는 것입니다.

>> 2. 여기에는 또 예외가 존재합니다. 3page에서 url을 잘 수집하다가 4page에는 [네이버뉴스]가 활성화 되있는 기사가 없어 컴퓨터가 5page의 마지막요소를 4page의 마지막요소로 읽습니다. 이렇게해서 수집이 종료된다면 7page까지 정보는 잃게되겠죠? 이는 elif 구문에 있는 것처럼 요소가 없다면 pass하도록 예외처리를 하였습니다.


#### 2. 뉴스 제목 및 본문 수집
- 위에서 수집한 url에 하나씩 접근하며 크롤링합니다. 
- 네이버는 크게 일반뉴스, 스포츠뉴스, 연예뉴스는 다른 UI를 제공하기에, 해당 페이지별로 코드를 구현할 필요가 있습니다. 이는 get_information함수로 구현하였습니다.
