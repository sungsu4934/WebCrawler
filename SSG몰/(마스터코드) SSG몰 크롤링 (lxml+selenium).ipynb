{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html import fromstring\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# headers\n",
    "ua = UserAgent()\n",
    "headers = {'User-Agent' : ua.random}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(f\"user-agent={ua}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['헬멧','바퀴']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL 크롤링\n",
    " - time.sleep(2)로 했음에도 401 ERROR 발생,, TIME을 충분히 줄 필요성이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['키워드명','url'])\n",
    "t1 = time.time()\n",
    "\n",
    "for j, KEYWORD in enumerate(KEYWORDS):\n",
    "\n",
    "    # 링크 제시\n",
    "    base_url = f'http://emart.ssg.com/search.ssg?target=all&query={KEYWORD}&sort=cnt&count=100' # count=100 --> 100개만 수집!\n",
    "    time.sleep(3)\n",
    "\n",
    "    # 정보수집 \n",
    "    res = requests.get(base_url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    res.encoding = \"utf-8-sig\"\n",
    "    parser = fromstring(res.text)\n",
    "\n",
    "    elements = parser.xpath('/html/body/div[4]/div[4]/div[2]/div[1]/div[5]/div[5]/div[1]/ul/li/div[2]/div[2]/div/a')\n",
    "    urls = ['http://emart.ssg.com'+element.get('href') for element in elements]\n",
    "    keywords = [KEYWORD] * len(urls)\n",
    "\n",
    "    # 데이터프레임에 추가\n",
    "    tmp = pd.DataFrame({'키워드명':keywords,\n",
    "                        'url':urls})\n",
    "    df = df.append(tmp, ignore_index=True)\n",
    "\n",
    "    # log\n",
    "    print(f'{j+1}번째 키워드 {KEYWORD} 완료!! == 경과시간: {round(time.time() - t1)}초 == 수집개수: {len(urls)}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 세부 값들 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드별로 url 을 리스트로 정리\n",
    "df2 = df[['키워드명']].drop_duplicates().reset_index(drop=True)\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "for index_ in range(df2.shape[0]):\n",
    "\n",
    "    keyword_tmp = df2.loc[index_, '키워드명']\n",
    "    urls_tmp = df.loc[(df['키워드명'] == keyword_tmp),'url'].tolist()\n",
    "\n",
    "    tmp_df = pd.DataFrame({'키워드명':[keyword_tmp],\n",
    "                          'url':[urls_tmp]})\n",
    "    df3 = df3.append(tmp_df, ignore_index=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 값들 초기화\n",
    "df_final = pd.DataFrame()\n",
    "driver = webdriver.Chrome('chromedriver_91.exe', options=options)\n",
    "t1 = time.time()\n",
    "NO_REVIEW = False\n",
    "\n",
    "for idx in range(df3.shape[0]):\n",
    "\n",
    "    for i ,url in enumerate(df3.loc[idx,'url']):\n",
    "\n",
    "        # url에 접근\n",
    "        try:\n",
    "            driver.get(url)   \n",
    "        except:\n",
    "            print('url에 접근할 수 없습니다. 다시 url을 open합니다. (10분휴식)')\n",
    "            driver.close()\n",
    "            time.sleep(600)\n",
    "            driver = webdriver.Chrome('chromedriver_91.exe', options=options)\n",
    "            driver.get(url)   \n",
    "\n",
    "\n",
    "        time.sleep(2) ### SSG가 자꾸 URL접속에 민감한 것 같아서 추가,,\n",
    "        print(f'{i+1}번째 url 시작!! == 경과시간: {round(time.time()-t1)}초')\n",
    "\n",
    "\n",
    "        # 제품명 정보수집\n",
    "        product = driver.find_element_by_class_name('cdtl_prd_info').find_element_by_class_name('cdtl_info_tit').text\n",
    "\n",
    "        # 스크롤을 어느정도 내린 후 고객리뷰 클릭\n",
    "        driver.execute_script(f\"window.scrollTo(0, 2000)\")     \n",
    "        time.sleep(0.5)\n",
    "        driver.find_element_by_xpath('//*[@id=\"_cdtl_dtlcont_wrap\"]/div[1]/div/div[1]/ul/li[3]/a').click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 고객리뷰 평점낮은순정렬\n",
    "        location = driver.find_element_by_xpath('//*[@id=\"cmt_select_sort\"]/div/a/span[2]')\n",
    "        driver.execute_script(f\"window.scrollTo(0, {location.location['y']-200})\") \n",
    "        driver.find_element_by_xpath('//*[@id=\"cmt_select_sort\"]/div/a/span[2]').click()\n",
    "        time.sleep(0.5)\n",
    "        driver.find_element_by_xpath('//*[@id=\"cmt_select_sort\"]/div/div/ul/li[4]/a/span').click()\n",
    "\n",
    "        # 리뷰관련 내부페이지 이동\n",
    "        while True:\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "            ### 정보수집\n",
    "            scores = [int(score.text) for score in driver.find_elements_by_xpath('//*[@id=\"cdtl_cmt_tbody\"]/tr/td[2]/div/span/span/span/em')]\n",
    "            dates = [date.text for date in driver.find_elements_by_xpath('//*[@id=\"cdtl_cmt_tbody\"]/tr/td[5]/div')]\n",
    "            writers = [writer.text for writer in driver.find_elements_by_xpath('//*[@id=\"cdtl_cmt_tbody\"]/tr/td[4]/div')]\n",
    "            contents = [content.text for content in driver.find_elements_by_xpath('//*[@id=\"cdtl_cmt_tbody\"]/tr/td[3]/div/a/div[1]/span')]\n",
    "\n",
    "            ### 데이터프레임화 및 추가\n",
    "            tmp = pd.DataFrame({'제품명':[product]*len(writers),\n",
    "                              '작성자':writers,\n",
    "                              '작성일자':dates,\n",
    "                              '평점':scores,\n",
    "                              '본문':contents,\n",
    "                              '검색키워드명':[df3.loc[idx,'키워드명']]*len(writers),\n",
    "                              'url':[url]*len(writers)})\n",
    "\n",
    "            df_final = df_final.append(tmp.drop_duplicates(), ignore_index=True)\n",
    "\n",
    "            ### 페이지있는 숫자bar로 이동\n",
    "            time.sleep(0.5)\n",
    "            location = driver.find_elements_by_xpath('//*[@id=\"comment_navi_area\"]/strong')\n",
    "\n",
    "            ##### 리뷰가 없다면, while문 탈출 및 키워드 이동\n",
    "            if len(location) == 0:\n",
    "                print('리뷰가 없습니다.')\n",
    "                NO_REVIEW = True\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                driver.execute_script(f\"window.scrollTo(0, {location[0].location['y']-200})\") \n",
    "\n",
    "            ### 페이지 이동\n",
    "            page_now = int(driver.find_element_by_xpath('//*[@id=\"comment_navi_area\"]/strong').text)\n",
    "            page_dict = {element.text : element for element in driver.find_elements_by_xpath('//*[@id=\"comment_navi_area\"]/a') if (element.text.isdigit()) | (element.text == '다음')}\n",
    "            move_page = page_now + 1\n",
    "\n",
    "            if '다음' in page_dict.keys():\n",
    "\n",
    "                if move_page % 10 == 1:\n",
    "                    page_dict['다음'].click()\n",
    "\n",
    "                else:\n",
    "                    page_dict[str(move_page)].click()\n",
    "\n",
    "            else:\n",
    "                if (len([int(key) for key in page_dict.keys()]) == 0):\n",
    "                    print('모든 페이지에 도달했습니다. 해당 페이지 리뷰 수집을 종료합니다.')\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    if (page_now == max([int(key) for key in page_dict.keys()])+1):\n",
    "                        print('모든 페이지에 도달했습니다. 해당 페이지 리뷰 수집을 종료합니다.')\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        page_dict[str(move_page)].click()\n",
    "\n",
    "            ### 수집종료조건: 평점 3이하만 수집한다.\n",
    "            if max(scores) >= 4:\n",
    "                print(f'{page_now} 페이지의 평점은 4점이상 입니다. 수집할 필요가 없다고 판단하여 종료합니다.')\n",
    "                break\n",
    "\n",
    "\n",
    "        if NO_REVIEW == True:\n",
    "            print('리뷰가 더이상 없네요. 키워드를 변경합니다.') \n",
    "            print('키워드가 바뀌었습니다. 5초를 쉽니다.')\n",
    "            time.sleep(5)\n",
    "            break\n",
    "\n",
    "\n",
    "    df_final = df_final[df_final['평점'] <= 3].drop_duplicates().reset_index(drop=True)\n",
    "    print('크롤링 완료!!')\n",
    "    \n",
    "df_final = df_final[df_final['평점'] <= 3].drop_duplicates().reset_index(drop=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('SSG_크롤링_샘플.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
